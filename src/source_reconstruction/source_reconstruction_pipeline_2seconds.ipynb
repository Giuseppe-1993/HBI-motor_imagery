{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pyvistaqt 3d backend.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import os,sys\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import mne\n",
    "mne.viz.set_3d_backend(\"pyvista\")\n",
    "from scipy.stats import sem\n",
    "from scipy.io import savemat, loadmat\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from mne.beamformer import make_lcmv, apply_lcmv, apply_lcmv_epochs\n",
    "import numpy as np\n",
    "\n",
    "from defintions import ROOT__DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortical Parcellation\n",
    "\n",
    "Using the Desikan-Killiany Atlas, comprising of 68 cortical regions bilaterally. \n",
    "\n",
    "- Paper: https://surfer.nmr.mgh.harvard.edu/ftp/articles/desikan06-parcellation.pdf\n",
    "\n",
    "- FreeSurfer: https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANATOMICAL PARCELLATION (LABELS)\n",
    "# the full list of cortical labels\n",
    "mylabel_names = ['bankssts-lh',\n",
    "                'bankssts-rh',\n",
    "                'caudalanteriorcingulate-lh',\n",
    "                'caudalanteriorcingulate-rh',\n",
    "                'caudalmiddlefrontal-lh',\n",
    "                'caudalmiddlefrontal-rh',\n",
    "                'cuneus-lh',\n",
    "                'cuneus-rh',\n",
    "                'entorhinal-lh',\n",
    "                'entorhinal-rh',\n",
    "                'frontalpole-lh',\n",
    "                'frontalpole-rh',\n",
    "                'fusiform-lh',\n",
    "                'fusiform-rh',\n",
    "                'inferiorparietal-lh',\n",
    "                'inferiorparietal-rh',\n",
    "                'inferiortemporal-lh',\n",
    "                'inferiortemporal-rh',\n",
    "                'insula-lh',\n",
    "                'insula-rh',\n",
    "                'isthmuscingulate-lh',\n",
    "                'isthmuscingulate-rh',\n",
    "                'lateraloccipital-lh',\n",
    "                'lateraloccipital-rh',\n",
    "                'lateralorbitofrontal-lh',\n",
    "                'lateralorbitofrontal-rh',\n",
    "                'lingual-lh',\n",
    "                'lingual-rh',\n",
    "                'medialorbitofrontal-lh',\n",
    "                'medialorbitofrontal-rh',\n",
    "                'middletemporal-lh',\n",
    "                'middletemporal-rh',\n",
    "                'paracentral-lh',\n",
    "                'paracentral-rh',\n",
    "                'parahippocampal-lh',\n",
    "                'parahippocampal-rh',\n",
    "                'parsopercularis-lh',\n",
    "                'parsopercularis-rh',\n",
    "                'parsorbitalis-lh',\n",
    "                'parsorbitalis-rh',\n",
    "                'parstriangularis-lh',\n",
    "                'parstriangularis-rh',\n",
    "                'pericalcarine-lh',\n",
    "                'pericalcarine-rh',\n",
    "                'postcentral-lh',\n",
    "                'postcentral-rh',\n",
    "                'posteriorcingulate-lh',\n",
    "                'posteriorcingulate-rh',\n",
    "                'precentral-lh',\n",
    "                'precentral-rh',\n",
    "                'precuneus-lh',\n",
    "                'precuneus-rh',\n",
    "                'rostralanteriorcingulate-lh',\n",
    "                'rostralanteriorcingulate-rh',\n",
    "                'rostralmiddlefrontal-lh',\n",
    "                'rostralmiddlefrontal-rh',\n",
    "                'superiorfrontal-lh',\n",
    "                'superiorfrontal-rh',\n",
    "                'superiorparietal-lh',\n",
    "                'superiorparietal-rh',\n",
    "                'superiortemporal-lh',\n",
    "                'superiortemporal-rh',\n",
    "                'supramarginal-lh',\n",
    "                'supramarginal-rh',\n",
    "                'temporalpole-lh',\n",
    "                'temporalpole-rh',\n",
    "                'transversetemporal-lh',\n",
    "                'transversetemporal-rh']\n",
    "\n",
    "# the region of interest (ROI) from the motor cortex. \n",
    "roi_labels = ['postcentral-lh',\n",
    "            'postcentral-rh',\n",
    "            'precentral-lh',\n",
    "            'precentral-rh']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Inverse models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/oem/Documents/HBI-motor_imagery/HBI-motor_imagery/data/epochs-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    4000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "49 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "0 files missing from root.txt in /home/oem/mne_data/MNE-fsaverage-data\n",
      "0 files missing from bem.txt in /home/oem/mne_data/MNE-fsaverage-data/fsaverage\n",
      "Leadfield size : 64 sensors x 61452 dipoles\n"
     ]
    }
   ],
   "source": [
    "fname = join(ROOT__DIR, 'data/epochs-epo.fif')\n",
    "cond = 'left' # 'right'\n",
    "\n",
    "\n",
    "epochs = mne.read_epochs(fname=fname, preload=True) # load the MI/ME EEG epochs\n",
    "epochs.resample(256.0) # down-sample to 256 Hz. \n",
    "epochs = epochs[cond]\n",
    "\n",
    "# Apply projection, as suggested by MNE-Python\n",
    "epochs = epochs.set_eeg_reference(projection=True).apply_proj()\n",
    "\n",
    "# Extract the Data Covariance and Noise Covariance Matrix\n",
    "data_tmin, data_tmax = (0.5, 2.0) # change to 4s for full-window length\n",
    "noise_tmin, noise_tmax = (-1.0, 0.0)\n",
    "crop_tmin, crop_tmax = (-1, 2.1)\n",
    "\n",
    "# Computing the covariance matrices\n",
    "data_cov = mne.compute_covariance(epochs, tmin=data_tmin, tmax=data_tmax, method='empirical', verbose=False)\n",
    "noise_cov = mne.compute_covariance(epochs, tmin=noise_tmin, tmax=noise_tmax, method='empirical', verbose=False)\n",
    "\n",
    "# Crop epochs in the time of interest\n",
    "cropped_epochs = epochs.crop(crop_tmin, crop_tmax) # Leave extra 100ms for smearing effects after wavelet\n",
    "\n",
    "del epochs # save memory\n",
    "\n",
    "########## MAKE FORWARD SOLUTION #################################\n",
    "subject = 'fsaverage'\n",
    "trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "src = os.path.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "bem = os.path.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif') # average BEM model\n",
    "\n",
    "fwd = mne.make_forward_solution(cropped_epochs.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0, n_jobs=-1, verbose=False) # standard params\n",
    "\n",
    "leadfield = fwd['sol']['data'] # get the leadfield matrix (nSensors-x-nSources)\n",
    "print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "\n",
    "fwd_fname = '' # file name to save the forward solution\n",
    "# mne.write_forward_solution(fwd_fname, fwd, overwrite=True, verbose=False) # uncomment to save the forward model\n",
    "\n",
    "src = fwd['src']\n",
    "\n",
    "########## INVERSE MODEL (LCMV Filters) ##########################\n",
    "filters = make_lcmv(cropped_epochs.info, fwd, data_cov, reg=0.05,\n",
    "                    noise_cov=noise_cov, pick_ori='max-power',\n",
    "                    weight_norm='unit-noise-gain', rank=None, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-courses in ROIs\n",
    "\n",
    "- This may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files missing from root.txt in /home/oem/mne_data/MNE-fsaverage-data\n",
      "0 files missing from bem.txt in /home/oem/mne_data/MNE-fsaverage-data/fsaverage\n",
      "Processing epoch : 1\n",
      "Processing epoch : 2\n",
      "Processing epoch : 3\n",
      "Processing epoch : 4\n",
      "Processing epoch : 5\n",
      "Processing epoch : 6\n",
      "Processing epoch : 7\n",
      "Processing epoch : 8\n",
      "Processing epoch : 9\n",
      "Processing epoch : 10\n",
      "Processing epoch : 11\n",
      "Processing epoch : 12\n",
      "Processing epoch : 13\n",
      "Processing epoch : 14\n",
      "Processing epoch : 15\n",
      "Processing epoch : 16\n",
      "Processing epoch : 17\n",
      "Processing epoch : 18\n",
      "Processing epoch : 19\n",
      "Processing epoch : 20\n",
      "Processing epoch : 21\n",
      "Processing epoch : 22\n",
      "Processing epoch : 23\n",
      "Processing epoch : 24\n",
      "[done]\n",
      "Processing epoch : 1\n",
      "Processing epoch : 2\n",
      "Processing epoch : 3\n",
      "Processing epoch : 4\n",
      "Processing epoch : 5\n",
      "Processing epoch : 6\n",
      "Processing epoch : 7\n",
      "Processing epoch : 8\n",
      "Processing epoch : 9\n",
      "Processing epoch : 10\n",
      "Processing epoch : 11\n",
      "Processing epoch : 12\n",
      "Processing epoch : 13\n",
      "Processing epoch : 14\n",
      "Processing epoch : 15\n",
      "Processing epoch : 16\n",
      "Processing epoch : 17\n",
      "Processing epoch : 18\n",
      "Processing epoch : 19\n",
      "Processing epoch : 20\n",
      "Processing epoch : 21\n",
      "Processing epoch : 22\n",
      "Processing epoch : 23\n",
      "Processing epoch : 24\n",
      "[done]\n",
      "Processing epoch : 1\n",
      "Processing epoch : 2\n",
      "Processing epoch : 3\n",
      "Processing epoch : 4\n",
      "Processing epoch : 5\n",
      "Processing epoch : 6\n",
      "Processing epoch : 7\n",
      "Processing epoch : 8\n",
      "Processing epoch : 9\n",
      "Processing epoch : 10\n",
      "Processing epoch : 11\n",
      "Processing epoch : 12\n",
      "Processing epoch : 13\n",
      "Processing epoch : 14\n",
      "Processing epoch : 15\n",
      "Processing epoch : 16\n",
      "Processing epoch : 17\n",
      "Processing epoch : 18\n",
      "Processing epoch : 19\n",
      "Processing epoch : 20\n",
      "Processing epoch : 21\n",
      "Processing epoch : 22\n",
      "Processing epoch : 23\n",
      "Processing epoch : 24\n",
      "[done]\n",
      "Processing epoch : 1\n",
      "Processing epoch : 2\n",
      "Processing epoch : 3\n",
      "Processing epoch : 4\n",
      "Processing epoch : 5\n",
      "Processing epoch : 6\n",
      "Processing epoch : 7\n",
      "Processing epoch : 8\n",
      "Processing epoch : 9\n",
      "Processing epoch : 10\n",
      "Processing epoch : 11\n",
      "Processing epoch : 12\n",
      "Processing epoch : 13\n",
      "Processing epoch : 14\n",
      "Processing epoch : 15\n",
      "Processing epoch : 16\n",
      "Processing epoch : 17\n",
      "Processing epoch : 18\n",
      "Processing epoch : 19\n",
      "Processing epoch : 20\n",
      "Processing epoch : 21\n",
      "Processing epoch : 22\n",
      "Processing epoch : 23\n",
      "Processing epoch : 24\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = os.path.dirname(fs_dir)\n",
    "\n",
    "# Get time-courses for all sources\n",
    "time_courses = {}\n",
    "for label in roi_labels:\n",
    "\n",
    "    labels_parc = mne.read_labels_from_annot(subject, parc='aparc', subjects_dir=subjects_dir, regexp=label, verbose=False)[0]\n",
    "\n",
    "    stcall = apply_lcmv_epochs(epochs=cropped_epochs, filters=filters, return_generator=True, verbose=False)\n",
    "\n",
    "    time_courses[label] = []\n",
    "\n",
    "    assert label == labels_parc.name\n",
    "    \n",
    "    for stc_epoch in stcall:\n",
    "\n",
    "        pca_anat = mne.extract_label_time_course(stc_epoch, labels_parc, src, mode='pca_flip', verbose=False)[0]\n",
    "        # flip the pca so that the max power between tmin and tmax is positive\n",
    "        pca_anat *= np.sign(pca_anat[np.argmax(np.abs(pca_anat))])  \n",
    "        # Append time course to list for this label         \n",
    "        time_courses[label].append(pca_anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Frequency Reconstruction\n",
    "\n",
    "- Apply baseline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a time vector from MNE\n",
    "tf_epochs, _ = mne.time_frequency.tfr_morlet(cropped_epochs, freqs=np.arange(8, 31, 1), output='power', n_cycles=5, use_fft=True, decim=3, n_jobs=1)\n",
    "time_vector = tf_epochs.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction (mode: zscore)\n",
      "Applying baseline correction (mode: zscore)\n",
      "Applying baseline correction (mode: zscore)\n",
      "Applying baseline correction (mode: zscore)\n"
     ]
    }
   ],
   "source": [
    "# Define a zeros matrix of shape nLabels, 2, nTime, where 2 is alpha and beta. \n",
    "srate = cropped_epochs.info['sfreq']\n",
    "\n",
    "tf_time_courses = np.zeros(shape=(len(roi_labels), 2, time_vector.shape[-1]))\n",
    "\n",
    "for label_idx, label in enumerate(roi_labels):\n",
    "\n",
    "    data = np.asarray(time_courses[label]) # Extract data from ROI into a numpy array\n",
    "\n",
    "    # TF decomposition using wavelet convolution\n",
    "    cued_tf = mne.time_frequency.tfr_array_morlet(data[:, None, :], sfreq=srate, freqs=np.arange(8, 31, 1), output='power', n_cycles=5, use_fft=True, decim=3, n_jobs=1)\n",
    "\n",
    "    cued_tf = cued_tf.mean(axis=0)\n",
    "\n",
    "    # apply baseline correction (average across trials is computed in the argument)\n",
    "    baseline_cued_tf = mne.baseline.rescale(data=cued_tf, times=time_vector, baseline=(-.500, 0), mode='zscore')[0, :, :]\n",
    "    alpha = baseline_cued_tf[:6, :].mean(axis=0) # average across alpha (8-13 Hz)\n",
    "    beta = baseline_cued_tf[6:, :].mean(axis=0) # average across beta\n",
    "    cued_tf = np.concatenate((alpha[None, :], beta[None, :]), axis=0)\n",
    "    tf_time_courses[label_idx, :, :] = cued_tf\n",
    "\n",
    "    # save numpy array if relevant\n",
    "    # np.save(fname, full_tf_time_courses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PhD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faaf0e7bf9a379e9bda35f73f64ea7f91eb94ee6c7d1154a01c68afd2a70e8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
